# GPU-enabled Dockerfile for Q-Structurize with StandardPdfPipeline
# Optimized for 2x NVIDIA H200 GPUs with CUDA 12.6
FROM nvidia/cuda:12.6.2-runtime-ubuntu22.04

WORKDIR /app

# Build-time environment variables (rarely change - keeps cache valid)
ENV PYTHONUNBUFFERED=1 \
    DEBIAN_FRONTEND=noninteractive \
    DOCLING_ARTIFACTS_PATH=/root/.cache/docling/models \
    HF_HOME=/app/.cache/huggingface \
    PATH="/usr/local/bin:${PATH}"

# Install Python 3.11 and system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
      software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update && apt-get install -y --no-install-recommends \
      python3.11 \
      python3.11-dev \
      python3.11-distutils \
      python3-pip \
      gcc g++ libgomp1 libgl1 libglib2.0-0 curl wget git \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN python -m pip install --no-cache-dir --upgrade pip setuptools wheel

# ---- cache-friendly boundary ----
# Install Python deps first (cacheable)
COPY requirements.txt .

# Install PyTorch with CUDA 12.6 support, then other requirements
RUN pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cu124 && \
    pip install --no-cache-dir -r requirements.txt

# Prepare cache directories (these will be volumes at runtime)
RUN mkdir -p /root/.cache/docling/models ${HF_HOME} /app/uploads

# (Optional) Pre-download models; will fill the mounted volume and persist
# Comment out if you prefer first-run download to populate the volume.
RUN echo "Pre-downloading Docling models (optional)..." && \
    docling-tools models download || true

# Verify (non-fatal if empty on fresh volume)
RUN ls -lah /root/.cache/docling/models || true

# Runtime environment variables (change these without invalidating cache layers above)
# GPU-optimized settings for H200
# 
# PyTorch/CUDA settings
ENV TOKENIZERS_PARALLELISM=false \
    CUDA_VISIBLE_DEVICES=0 \
    PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Threading configuration for CPU operations
ENV OMP_NUM_THREADS=64 \
    MKL_NUM_THREADS=64 \
    OPENBLAS_NUM_THREADS=64 \
    NUMEXPR_NUM_THREADS=32 \
    TORCH_NUM_THREADS=64 \
    KMP_BLOCKTIME=1 \
    KMP_SETTINGS=1 \
    KMP_AFFINITY="granularity=fine,compact,1,0" \
    MALLOC_ARENA_MAX=2

# DOCLING CORE CONFIGURATION
# OCR Configuration: Enable OCR to extract text from scanned documents
# OCR Languages: Comma-separated list (e.g., "en,fr,de")
# Table Extraction: Enable table structure detection and extraction
# Table Mode: "fast" for speed, "accurate" for better quality
# Cell Matching: Match table predictions back to PDF cells (can break if cells are merged)
# Code Enrichment: Extract code blocks with OCR
# Formula Enrichment: Extract formulas and return LaTeX code
# Picture Classification: Classify images (diagram, photo, chart, etc.)
ENV DOCLING_ENABLE_OCR=false \
    DOCLING_OCR_LANGUAGES=en \
    DOCLING_DO_TABLE_STRUCTURE=true \
    DOCLING_TABLE_MODE=accurate \
    DOCLING_DO_CELL_MATCHING=true \
    DOCLING_DO_CODE_ENRICHMENT=false \
    DOCLING_DO_FORMULA_ENRICHMENT=false \
    DOCLING_DO_PICTURE_CLASSIFICATION=true

# PICTURE DESCRIPTION SETTINGS (enabled via parse_images=true API parameter)
# Picture Description Model: "smolvlm", "granite", "api", or "custom"
# Picture Description Prompt: Custom prompt for image description models
# Picture Area Threshold: Minimum percentage of page area (0.05 = 5%) for image to be processed
# Picture Batch Size: Number of images processed per batch
# Picture Max New Tokens: Maximum tokens generated by VLM for image descriptions
# Picture Scale: Image upscaling factor before processing (2.0 = 2x)
# Picture Provenance: Custom label for image descriptions
# Picture Concurrency: Parallel API calls for API-based models (API mode only)
# API-based Picture Description: Alternative to local VLM models
ENV DOCLING_PICTURE_DESCRIPTION_MODEL=granite \
    DOCLING_PICTURE_DESCRIPTION_PROMPT="Provide a detailed, objective description of the image suitable for embedding generation and accurate semantic retrieval during RFP analysis. Include key visual elements, text, layout, relationships between objects, business-relevant components, and any contextual cues that could help match this image to RFP requirements or use cases. Avoid stylistic language or irrelevant commentary." \
    DOCLING_PICTURE_AREA_THRESHOLD=0.05 \
    DOCLING_PICTURE_BATCH_SIZE=8 \
    DOCLING_PICTURE_MAX_NEW_TOKENS=512 \
    DOCLING_PICTURE_SCALE=2.0 \
    DOCLING_PICTURE_PROVENANCE="Granite Vision" \
    DOCLING_PICTURE_CONCURRENCY=8 \
    DOCLING_PICTURE_DESCRIPTION_API_URL=https://api.openai.com/v1/chat/completions \
    DOCLING_PICTURE_DESCRIPTION_API_KEY="" \
    DOCLING_PICTURE_DESCRIPTION_API_MODEL=gpt-4o \
    DOCLING_PICTURE_DESCRIPTION_API_TIMEOUT=90

# PERFORMANCE & BATCHING CONFIGURATION
# Layout Batch Size: Number of pages processed together in layout detection stage
# OCR Batch Size: Number of pages processed together in OCR stage
# Table Batch Size: Number of pages processed together in table extraction stage
# Page Batch Size: Global setting for number of pages processed in one batch
# Queue Max Size: Maximum size of internal processing queues (backpressure control)
# Batch Polling Interval: Seconds between worker thread checks for new work (0.5 = 500ms)
# Accelerator Device: "cpu", "cuda", "cuda:0", "cuda:1", "mps", or "auto"
ENV DOCLING_LAYOUT_BATCH_SIZE=64 \
    DOCLING_OCR_BATCH_SIZE=64 \
    DOCLING_TABLE_BATCH_SIZE=64 \
    DOCLING_PAGE_BATCH_SIZE=24 \
    DOCLING_QUEUE_MAX_SIZE=2000 \
    DOCLING_BATCH_POLLING_INTERVAL=0.5 \
    DOCLING_ACCELERATOR_DEVICE=cuda

# GLOBAL PERFORMANCE SETTINGS
# Doc Batch Size: Number of documents processed in one batch (default: 1)
# Doc Batch Concurrency: Parallel threads processing documents (experimental, requires free-threaded Python)
# Elements Batch Size: Number of elements processed in enrichment models (default: 16)
ENV DOCLING_DOC_BATCH_SIZE=1 \
    DOCLING_DOC_BATCH_CONCURRENCY=1 \
    DOCLING_ELEMENTS_BATCH_SIZE=16

# DOCUMENT SAFETY LIMITS
# Max Num Pages: Maximum number of pages to process per document (prevents resource exhaustion)
# Max File Size: Maximum file size in bytes (104857600 = 100MB)
# Page Range: Process specific page range, format "start-end" (e.g., "1-100") or empty for all pages
# Document Timeout: Maximum seconds to process a document before timeout (300 = 5 minutes)
ENV DOCLING_MAX_NUM_PAGES=1000 \
    DOCLING_MAX_FILE_SIZE=104857600 \
    DOCLING_PAGE_RANGE="" \
    DOCLING_DOCUMENT_TIMEOUT=300

# LAYOUT MODEL SELECTION
# Layout Model: "heron" (default), "heron_101", "egret_medium", "egret_large", "egret_xlarge"
ENV DOCLING_LAYOUT_MODEL=heron

# IMAGE GENERATION OPTIONS
# Generate Page Images: Enable generation of full page images (increases memory usage)
# Generate Picture Images: Enable generation of individual picture images (increases memory usage)
# Images Scale: Scale factor for generated images (1.0 = original size)
ENV DOCLING_GENERATE_PAGE_IMAGES=false \
    DOCLING_GENERATE_PICTURE_IMAGES=false \
    DOCLING_IMAGES_SCALE=1.0

# VLM INTEGRATION
# Force Backend Text: Use PDF backend text instead of generated text (for VLM-only processing)
ENV DOCLING_FORCE_BACKEND_TEXT=false

# DEBUG & PROFILING
# Enable Profiling: Enable detailed pipeline timing profiling (adds overhead, set to "true" for debugging)
ENV DOCLING_ENABLE_PROFILING=false \
    LOG_LEVEL=DEBUG

# Finally copy app sources (changes here don't invalidate pip layer)
COPY . .

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]

